# Service configuration for canoe.
apiVersion: v1
kind: Service
metadata:
  name: canoe
  labels:
    app: canoe
spec:
  clusterIP: None  # ClusterIP set to None for headless service.
  ports:
  - name: nccl  # Port for torchrun master-worker node communication.
    port: 29500
    targetPort: 29500
  selector:
    job-name: canoe  # Selector for pods associated with this service.

---

# Job configuration for canoe training.
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: canoe  # Label for identifying the pods to the Service.
  name: canoe
spec:
  completionMode: Indexed
  completions: 2  # Should match the number of nodes.
  parallelism: 2  # Should match the number of nodes.
  template:
    metadata:
      annotations:
        linkerd.io/inject: disabled  # Disable service mesh as it might interfere with networking.
    spec:
      containers:
      - name: canoe
        image: luminoctum/ubuntu22.04-cuda12.9-py3.10-canoe:2026-01-31b
        imagePullPolicy: IfNotPresent

        ports:
        - name: nccl
          containerPort: 29500

        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace

        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name

        - name: MASTER_PORT
          value: '29500'
        - name: NNODES
          value: '2'  # Number of training nodes.
        - name: NGPUS
          value: '1'  # Number of GPUs in the machine.

        command: ["/bin/bash", "-lc"]
        args:
        - |
          MASTER_ADDR=${POD_NAME}.canoe.${NAMESPACE}
          echo "$MASTER_ADDR"

          torchrun \
            --nproc_per_node $NGPUS \
            --nnodes $NNODES \
            --node_rank $JOB_COMPLETION_INDEX \
            --master_addr $MASTER_ADDR \
            --master_port $MASTER_PORT \
            straka.py

        resources:
          limits:
            nvidia.com/gpu: 1
            cpu: "2"
            memory: "20Gi"
          requests:
            nvidia.com/gpu: 1
            cpu: "2"
            memory: "20Gi"

        volumeMounts:
        - name: dshm
          mountPath: /dev/shm  # Mounting emptyDir as shared memory for communication within nodes.
        - name: model-data
          mountPath: /data

      restartPolicy: Never
      subdomain: canoe  # Required for pod-to-pod communication in Indexed-Jobs.
      volumes:
      - name: model-data
        hostPath:
          path: /home/chengcli/data
          type: Directory
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 8Gi

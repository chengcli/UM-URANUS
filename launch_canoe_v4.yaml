apiVersion: v1
kind: Service
metadata:
  name: canoe
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  selector:
    app: canoe
  ports:
    - name: rdzv
      port: 29500
      targetPort: 29500
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: canoe
spec:
  serviceName: canoe
  replicas: 2
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: canoe
  template:
    metadata:
      labels:
        app: canoe
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 30

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                      - csrwks2024-0242.engin.umich.edu
                      - csrwks2024-0243.engin.umich.edu
                      - csrwks2024-0244.engin.umich.edu
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: canoe
              topologyKey: kubernetes.io/hostname

      containers:
        - name: canoe
          image: luminoctum/ubuntu22.04-cuda12.9-py3.10-canoe:2026-01-31b
          imagePullPolicy: IfNotPresent

          ports:
            - name: rdzv
              containerPort: 29500

          resources:
            requests:
              nvidia.com/gpu: 1
              cpu: "2"
              memory: "20Gi"
            limits:
              nvidia.com/gpu: 1
              cpu: "2"
              memory: "20Gi"

          env:
            # Optional: limit to the two GPUs that k8s assigns (usually 0,1 in-container)
            - name: CUDA_VISIBLE_DEVICES
              value: "0,1,2,3"

            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace

          volumeMounts:
            - name: model-data
              mountPath: /data
            - name: dshm
              mountPath: /dev/shm

          command: ["/bin/bash", "-lc"]
          args:
            - |
              set -euo pipefail

              NODE_RANK="${HOSTNAME##*-}"
              RDZV_ENDPOINT="canoe-0.canoe.${NAMESPACE}.svc:29500"
              RDZV_ID="canoe-2026-01-31"

              echo "HOSTNAME=${HOSTNAME}"
              echo "NODE_RANK=${NODE_RANK}"
              echo "RDZV_ENDPOINT=${RDZV_ENDPOINT}"
              echo "CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}"

              torchrun \
                --nnodes=2 \
                --nproc_per_node=1 \
                --node_rank="${NODE_RANK}" \
                --rdzv_backend=c10d \
                --rdzv_endpoint="${RDZV_ENDPOINT}" \
                --rdzv_id="${RDZV_ID}" \
                straka.py

      volumes:
        - name: model-data
          hostPath:
            path: /home/chengcli/data
            type: Directory
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: "8Gi"
